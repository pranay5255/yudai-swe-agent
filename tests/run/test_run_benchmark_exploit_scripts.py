from __future__ import annotations

import argparse
import importlib.util
import json
import os
import sys
from pathlib import Path
from unittest.mock import patch

import exploit_generation.benchmark as benchmark_mod
import exploit_generation.benchmark_episode as benchmark_episode_mod
import pytest

from exploit_generation.models import BenchmarkCase, ExploitResult


def _load_script_module(script_name: str):
    repo_root = Path(__file__).resolve().parents[2]
    script_path = repo_root / "scripts" / script_name
    module_name = f"tests_{script_path.stem}"
    spec = importlib.util.spec_from_file_location(module_name, script_path)
    assert spec is not None
    assert spec.loader is not None
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


def _make_case() -> BenchmarkCase:
    return BenchmarkCase(
        case_name="unit_case",
        task_source="unit",
        chain="mainnet",
        fork_block_number=12345678,
        target_contract_address="0x1111111111111111111111111111111111111111",
        source_code="pragma solidity ^0.8.19; contract Target {}",
    )


def _run_main_once(module, monkeypatch, tmp_path: Path, env_overrides: dict[str, str], output_dir: Path):
    benchmark_csv = tmp_path / "benchmark.csv"
    benchmark_csv.write_text("dummy\n")
    env_file = tmp_path / ".env"
    env_file.write_text("DUMMY=1\n")

    args = argparse.Namespace(
        index=None,
        case=None,
        chain=None,
        limit=1,
        output=None,
        config=None,
        model=None,
        image=None,
        cost_limit=None,
        no_yolo=False,
        interactive=None,
        env_file=env_file,
        benchmark_csv=benchmark_csv,
        cache_dir=tmp_path / "cache",
        verbose=False,
    )
    monkeypatch.setattr(module, "parse_args", lambda: args)
    monkeypatch.setattr(module.signal, "signal", lambda *_args, **_kwargs: None)

    case = _make_case()
    monkeypatch.setattr(benchmark_mod, "load_benchmark", lambda _path: [case])
    monkeypatch.setattr(benchmark_mod, "filter_by_chain", lambda cases, _chain: cases)
    monkeypatch.setattr(
        benchmark_mod,
        "get_case_by_name",
        lambda cases, name: next((c for c in cases if c.case_name == name), None),
    )
    monkeypatch.setattr(benchmark_mod, "enrich_case_with_source", lambda _case, cache_dir=None: _case)

    monkeypatch.setattr(benchmark_episode_mod, "configure_logging", lambda verbose=False: None)
    monkeypatch.setattr(benchmark_episode_mod, "load_env", lambda env_file=None: None)
    monkeypatch.setattr(
        benchmark_episode_mod,
        "get_rpc_urls",
        lambda: {
            "mainnet": "https://eth-mainnet.g.alchemy.com/v2/test-key-1234567890",
            "bsc": "https://bsc.example",
        },
    )

    captured: dict[str, object] = {}

    def fake_run_episode(config):
        captured["config"] = config
        return ExploitResult(
            episode_id=config.episode_id or "ep-unit",
            contract_name="Target",
            target_address=config.case.target_contract_address,
            success=True,
            profit_native_token=1.0,
            iterations=1,
            total_cost_usd=0.01,
        )

    monkeypatch.setattr(benchmark_episode_mod, "run_benchmark_exploit_episode", fake_run_episode)

    with patch.dict(os.environ, env_overrides, clear=False):
        rc = module.main()

    summary_files = sorted(output_dir.glob("benchmark_*.json"))
    assert len(summary_files) == 1
    summary = json.loads(summary_files[0].read_text())
    return rc, captured, summary


@pytest.mark.parametrize(
    "script_name",
    ["run_benchmark_exploit.py", "run_benchmark_exploit_v2.py"],
)
def test_parse_args_defaults_to_auto_interactive(script_name, monkeypatch):
    module = _load_script_module(script_name)
    monkeypatch.setattr(sys, "argv", [script_name])
    args = module.parse_args()
    assert args.interactive is None


def test_v2_main_prefers_v2_env_defaults_and_sets_v2_script_label(monkeypatch, tmp_path):
    module = _load_script_module("run_benchmark_exploit_v2.py")
    out_v1 = tmp_path / "out_v1"
    out_v2 = tmp_path / "out_v2"

    rc, captured, summary = _run_main_once(
        module,
        monkeypatch,
        tmp_path,
        env_overrides={
            "OPENROUTER_API_KEY": "test-key",
            "OPENROUTER_MODEL_NAME": "test-model",
            "EXPLOIT_OUTPUT_DIR": str(out_v1),
            "EXPLOIT_OUTPUT_DIR_V2": str(out_v2),
            "EXPLOIT_CONFIG": "benchmark_exploit.yaml",
            "EXPLOIT_CONFIG_V2": "benchmark_exploit_v2.yaml",
            "EXPLOIT_COST_LIMIT": "10.0",
            "EXPLOIT_COST_LIMIT_V2": "22.5",
        },
        output_dir=out_v2,
    )

    assert rc == 0
    assert summary["script"] == "scripts/run_benchmark_exploit_v2.py"
    assert summary["settings"]["output_dir"] == str(out_v2)
    assert summary["settings"]["config_path"] == "benchmark_exploit_v2.yaml"
    assert summary["settings"]["cost_limit"] == 22.5
    cfg = captured["config"]
    assert cfg.interactive is None
    assert cfg.yolo is True


def test_v1_main_uses_non_v2_env_defaults(monkeypatch, tmp_path):
    module = _load_script_module("run_benchmark_exploit.py")
    out_v1 = tmp_path / "out_v1"
    out_v2 = tmp_path / "out_v2"

    rc, _, summary = _run_main_once(
        module,
        monkeypatch,
        tmp_path,
        env_overrides={
            "OPENROUTER_API_KEY": "test-key",
            "OPENROUTER_MODEL_NAME": "test-model",
            "EXPLOIT_OUTPUT_DIR": str(out_v1),
            "EXPLOIT_OUTPUT_DIR_V2": str(out_v2),
            "EXPLOIT_CONFIG": "benchmark_exploit.yaml",
            "EXPLOIT_CONFIG_V2": "benchmark_exploit_v2.yaml",
            "EXPLOIT_COST_LIMIT": "9.5",
            "EXPLOIT_COST_LIMIT_V2": "22.5",
        },
        output_dir=out_v1,
    )

    assert rc == 0
    assert summary["script"] == "scripts/run_benchmark_exploit.py"
    assert summary["settings"]["output_dir"] == str(out_v1)
    assert summary["settings"]["config_path"] == "benchmark_exploit.yaml"
    assert summary["settings"]["cost_limit"] == 9.5
