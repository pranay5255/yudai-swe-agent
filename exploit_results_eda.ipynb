{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploit Results - Exploratory Data Analysis\n",
    "\n",
    "This notebook analyzes the benchmark results from the yudai-swe-agent exploit generation runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all main benchmark JSON files (excluding trajectory and result files)\n",
    "files = sorted(glob.glob('exploit_results/benchmark_*.json'))\n",
    "files = [f for f in files if '.traj.' not in f and '.result.' not in f]\n",
    "\n",
    "print(f\"Found {len(files)} benchmark result files\")\n",
    "\n",
    "# Load all data\n",
    "all_data = []\n",
    "for file_path in files:\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        all_data.append(data)\n",
    "\n",
    "print(f\"Loaded {len(all_data)} benchmark runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a flat dataframe with all cases\n",
    "records = []\n",
    "\n",
    "for run in all_data:\n",
    "    run_id = run.get('run_id', '')\n",
    "    run_status = run.get('status', 'unknown')\n",
    "    model = run.get('settings', {}).get('model_name', 'unknown')\n",
    "    cost_limit = run.get('settings', {}).get('cost_limit', None)\n",
    "    \n",
    "    started_at = run.get('started_at')\n",
    "    finished_at = run.get('finished_at')\n",
    "    \n",
    "    for case in run.get('cases', []):\n",
    "        record = {\n",
    "            'run_id': run_id,\n",
    "            'run_status': run_status,\n",
    "            'model': model,\n",
    "            'cost_limit': cost_limit,\n",
    "            'started_at': started_at,\n",
    "            'finished_at': finished_at,\n",
    "            'case_name': case.get('case_name', ''),\n",
    "            'chain': case.get('chain', 'unknown'),\n",
    "            'fork_block_number': case.get('fork_block_number'),\n",
    "            'status': case.get('status', 'unknown'),\n",
    "            'duration_sec': case.get('duration_sec'),\n",
    "            'episode_id': case.get('episode_id', ''),\n",
    "            'error_type': case.get('error', {}).get('type') if case.get('error') else None,\n",
    "            'error_message': case.get('error', {}).get('message') if case.get('error') else None,\n",
    "            'source_length': case.get('source_length'),\n",
    "            'metrics': case.get('metrics', {})\n",
    "        }\n",
    "        records.append(record)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Convert datetime columns\n",
    "df['started_at'] = pd.to_datetime(df['started_at'])\n",
    "df['finished_at'] = pd.to_datetime(df['finished_at'])\n",
    "df['date'] = df['started_at'].dt.date\n",
    "\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"BASIC STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal benchmark runs: {len(all_data)}\")\n",
    "print(f\"Total cases executed: {len(df)}\")\n",
    "print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nUnique models tested: {df['model'].nunique()}\")\n",
    "print(f\"Unique chains: {df['chain'].nunique()}\")\n",
    "print(f\"Unique cases: {df['case_name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Status Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Status distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Run status\n",
    "run_status_counts = df['run_status'].value_counts()\n",
    "axes[0].pie(run_status_counts.values, labels=run_status_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Run Status Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Case status\n",
    "case_status_counts = df['status'].value_counts()\n",
    "axes[1].pie(case_status_counts.values, labels=case_status_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Case Status Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRun Status:\")\n",
    "print(run_status_counts)\n",
    "print(\"\\nCase Status:\")\n",
    "print(case_status_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model usage and performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Model usage count\n",
    "model_counts = df['model'].value_counts()\n",
    "axes[0, 0].barh(model_counts.index, model_counts.values, color='steelblue')\n",
    "axes[0, 0].set_xlabel('Number of Runs')\n",
    "axes[0, 0].set_title('Model Usage Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Status by model\n",
    "status_by_model = pd.crosstab(df['model'], df['status'])\n",
    "status_by_model.plot(kind='bar', stacked=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Status Distribution by Model', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Model')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].legend(title='Status')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Average duration by model\n",
    "avg_duration = df.groupby('model')['duration_sec'].mean().sort_values(ascending=False)\n",
    "axes[1, 0].barh(avg_duration.index, avg_duration.values, color='coral')\n",
    "axes[1, 0].set_xlabel('Average Duration (seconds)')\n",
    "axes[1, 0].set_title('Average Case Duration by Model', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Success rate by model (excluding interrupted)\n",
    "completed_df = df[df['status'].isin(['completed', 'failed', 'success'])]\n",
    "if len(completed_df) > 0:\n",
    "    model_success = completed_df.groupby('model')['status'].apply(\n",
    "        lambda x: (x == 'success').sum() / len(x) * 100\n",
    "    ).sort_values(ascending=False)\n",
    "    axes[1, 1].barh(model_success.index, model_success.values, color='lightgreen')\n",
    "    axes[1, 1].set_xlabel('Success Rate (%)')\n",
    "    axes[1, 1].set_title('Success Rate by Model (Non-Interrupted)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No completed runs', ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    axes[1, 1].set_title('Success Rate by Model', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chain Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain distribution and performance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Chain distribution\n",
    "chain_counts = df['chain'].value_counts()\n",
    "axes[0].bar(chain_counts.index, chain_counts.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[0].set_xlabel('Chain')\n",
    "axes[0].set_ylabel('Number of Cases')\n",
    "axes[0].set_title('Cases by Blockchain', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Status by chain\n",
    "status_by_chain = pd.crosstab(df['chain'], df['status'])\n",
    "status_by_chain.plot(kind='bar', stacked=True, ax=axes[1])\n",
    "axes[1].set_title('Status Distribution by Chain', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Chain')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend(title='Status')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Average duration by chain\n",
    "avg_duration_chain = df.groupby('chain')['duration_sec'].mean().sort_values(ascending=False)\n",
    "axes[2].bar(avg_duration_chain.index, avg_duration_chain.values, color=['#95E1D3', '#F38181'])\n",
    "axes[2].set_xlabel('Chain')\n",
    "axes[2].set_ylabel('Average Duration (seconds)')\n",
    "axes[2].set_title('Average Duration by Chain', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nChain Statistics:\")\n",
    "print(df.groupby('chain').agg({\n",
    "    'duration_sec': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'source_length': ['mean', 'std']\n",
    "}).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Duration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Histogram of durations\n",
    "axes[0, 0].hist(df['duration_sec'].dropna(), bins=30, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Duration (seconds)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Case Durations', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].axvline(df['duration_sec'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"duration_sec\"].mean():.2f}s')\n",
    "axes[0, 0].axvline(df['duration_sec'].median(), color='green', linestyle='--', label=f'Median: {df[\"duration_sec\"].median():.2f}s')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot by model\n",
    "df.boxplot(column='duration_sec', by='model', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Duration Distribution by Model', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Model')\n",
    "axes[0, 1].set_ylabel('Duration (seconds)')\n",
    "plt.sca(axes[0, 1])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Box plot by chain\n",
    "df.boxplot(column='duration_sec', by='chain', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Duration Distribution by Chain', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Chain')\n",
    "axes[1, 0].set_ylabel('Duration (seconds)')\n",
    "\n",
    "# Box plot by status\n",
    "df.boxplot(column='duration_sec', by='status', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Duration Distribution by Status', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Status')\n",
    "axes[1, 1].set_ylabel('Duration (seconds)')\n",
    "plt.sca(axes[1, 1])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDuration Statistics:\")\n",
    "print(df['duration_sec'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error type analysis\n",
    "error_df = df[df['error_type'].notna()].copy()\n",
    "\n",
    "if len(error_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Error type distribution\n",
    "    error_counts = error_df['error_type'].value_counts()\n",
    "    axes[0].barh(error_counts.index, error_counts.values, color='salmon')\n",
    "    axes[0].set_xlabel('Count')\n",
    "    axes[0].set_title('Error Type Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Error types by model\n",
    "    error_by_model = pd.crosstab(error_df['model'], error_df['error_type'])\n",
    "    error_by_model.plot(kind='bar', stacked=True, ax=axes[1])\n",
    "    axes[1].set_title('Error Types by Model', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Model')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].legend(title='Error Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nError Type Summary:\")\n",
    "    print(error_counts)\n",
    "    \n",
    "    print(\"\\nSample Error Messages:\")\n",
    "    for error_type in error_counts.head(3).index:\n",
    "        sample = error_df[error_df['error_type'] == error_type]['error_message'].iloc[0]\n",
    "        print(f\"\\n{error_type}:\")\n",
    "        print(f\"  {sample[:200]}...\" if len(sample) > 200 else f\"  {sample}\")\nelse:\n",
    "    print(\"No errors found in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Runs over time\n",
    "runs_per_day = df.groupby('date').size()\n",
    "axes[0].plot(runs_per_day.index, runs_per_day.values, marker='o', linewidth=2, markersize=8)\n",
    "axes[0].fill_between(runs_per_day.index, runs_per_day.values, alpha=0.3)\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Number of Cases')\n",
    "axes[0].set_title('Benchmark Runs Over Time', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Status over time\n",
    "status_over_time = df.groupby(['date', 'status']).size().unstack(fill_value=0)\n",
    "status_over_time.plot(kind='area', stacked=True, ax=axes[1], alpha=0.7)\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Number of Cases')\n",
    "axes[1].set_title('Status Distribution Over Time', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(title='Status')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Source Code Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source length analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Distribution of source lengths\n",
    "axes[0].hist(df['source_length'].dropna(), bins=30, color='lightcoral', edgecolor='black')\n",
    "axes[0].set_xlabel('Source Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Source Code Length', fontsize=12, fontweight='bold')\n",
    "axes[0].axvline(df['source_length'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"source_length\"].mean():.0f}')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Source length vs duration scatter\n",
    "valid_data = df[df['source_length'].notna() & df['duration_sec'].notna()]\n",
    "axes[1].scatter(valid_data['source_length'], valid_data['duration_sec'], alpha=0.5, c=valid_data['status'].astype('category').cat.codes, cmap='viridis')\n",
    "axes[1].set_xlabel('Source Length (characters)')\n",
    "axes[1].set_ylabel('Duration (seconds)')\n",
    "axes[1].set_title('Source Length vs Duration', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Average source length by chain\n",
    "avg_source_length = df.groupby('chain')['source_length'].mean().sort_values(ascending=False)\n",
    "axes[2].bar(avg_source_length.index, avg_source_length.values, color=['#FFA07A', '#20B2AA'])\n",
    "axes[2].set_xlabel('Chain')\n",
    "axes[2].set_ylabel('Average Source Length')\n",
    "axes[2].set_title('Average Source Code Length by Chain', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSource Length Statistics:\")\n",
    "print(df['source_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Case-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top cases by frequency\n",
    "case_counts = df['case_name'].value_counts().head(15)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Most tested cases\n",
    "axes[0].barh(case_counts.index, case_counts.values, color='mediumseagreen')\n",
    "axes[0].set_xlabel('Number of Runs')\n",
    "axes[0].set_title('Top 15 Most Tested Cases', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Average duration per case (top 15)\n",
    "avg_duration_case = df.groupby('case_name')['duration_sec'].mean().sort_values(ascending=False).head(15)\n",
    "axes[1].barh(avg_duration_case.index, avg_duration_case.values, color='mediumpurple')\n",
    "axes[1].set_xlabel('Average Duration (seconds)')\n",
    "axes[1].set_title('Top 15 Cases by Average Duration', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal unique cases: {df['case_name'].nunique()}\")\n",
    "print(f\"\\nMost tested cases:\")\n",
    "print(case_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive summary\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Overall Statistics:\")\n",
    "print(f\"  ‚Ä¢ Total benchmark runs: {len(all_data)}\")\n",
    "print(f\"  ‚Ä¢ Total cases executed: {len(df)}\")\n",
    "print(f\"  ‚Ä¢ Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"  ‚Ä¢ Unique cases: {df['case_name'].nunique()}\")\n",
    "print(f\"  ‚Ä¢ Unique models: {df['model'].nunique()}\")\n",
    "print(f\"  ‚Ä¢ Unique chains: {df['chain'].nunique()}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Status Breakdown:\")\n",
    "for status, count in df['status'].value_counts().items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  ‚Ä¢ {status}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nü§ñ Model Usage:\")\n",
    "for model, count in df['model'].value_counts().items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  ‚Ä¢ {model}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚õìÔ∏è Chain Distribution:\")\n",
    "for chain, count in df['chain'].value_counts().items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  ‚Ä¢ {chain}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Duration Statistics:\")\n",
    "print(f\"  ‚Ä¢ Mean: {df['duration_sec'].mean():.2f} seconds\")\n",
    "print(f\"  ‚Ä¢ Median: {df['duration_sec'].median():.2f} seconds\")\n",
    "print(f\"  ‚Ä¢ Min: {df['duration_sec'].min():.2f} seconds\")\n",
    "print(f\"  ‚Ä¢ Max: {df['duration_sec'].max():.2f} seconds\")\n",
    "print(f\"  ‚Ä¢ Total time: {df['duration_sec'].sum() / 3600:.2f} hours\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Error Statistics:\")\n",
    "error_counts = df['error_type'].value_counts()\n",
    "if len(error_counts) > 0:\n",
    "    for error_type, count in error_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  ‚Ä¢ {error_type}: {count} ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"  ‚Ä¢ No errors recorded\")\n",
    "\n",
    "print(f\"\\nüìù Source Code Statistics:\")\n",
    "print(f\"  ‚Ä¢ Mean length: {df['source_length'].mean():.0f} characters\")\n",
    "print(f\"  ‚Ä¢ Median length: {df['source_length'].median():.0f} characters\")\n",
    "print(f\"  ‚Ä¢ Min length: {df['source_length'].min():.0f} characters\")\n",
    "print(f\"  ‚Ä¢ Max length: {df['source_length'].max():.0f} characters\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Detailed DataFrame View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the full dataframe\n",
    "print(\"Full DataFrame:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary to CSV for further analysis\n",
    "output_file = 'exploit_results/benchmark_summary.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Summary exported to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
